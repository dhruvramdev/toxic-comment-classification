{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 10)\n",
      "(153164, 4)\n",
      "(153164, 7)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./preprocessed/train.csv')\n",
    "test = pd.read_csv('./preprocessed/test.csv')\n",
    "test_labels = pd.read_csv('./dataset/test_labels.csv')\n",
    "\n",
    "test = test.fillna('')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf(**kwargs):\n",
    "    return LogisticRegression(C=2, solver='lbfgs', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, classifier, **kwargs):\n",
    "    test_cols = [ 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    roc_aoc_scores = []\n",
    "    clfs = []\n",
    "    \n",
    "    for eval_col in test_cols:\n",
    "#         print(\"FIT \", eval_col)\n",
    "        y = train[eval_col]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.3, random_state=101)\n",
    "\n",
    "        clf = classifier(**kwargs).fit(X_train, y_train)\n",
    "        predicted= clf.predict_proba(X_test)[:,1]\n",
    "            \n",
    "        roc_aoc_scores.append(metrics.roc_auc_score(y_test, predicted))\n",
    "        clfs.append(clf)\n",
    "    \n",
    "    avg_score = np.mean(np.array(roc_aoc_scores))\n",
    "    for index, score in enumerate(roc_aoc_scores) :\n",
    "        print(\"Score for {:<10} is {:<10}\".format(test_cols[index], score))\n",
    "    print(\"Score :\" , avg_score)\n",
    "    return clfs, roc_aoc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_regex = r'[a-zA-Z0-9]+'\n",
    "tokenizer = RegexpTokenizer(token_regex).tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, strip_accents='unicode', ngram_range = (1,2), tokenizer = tokenizer, max_features=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tfidf.fit_transform(train['comment_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features :  100000\n"
     ]
    }
   ],
   "source": [
    "num_features = len(tfidf.get_feature_names())\n",
    "print(\"Num Features : \", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for toxic      is 0.975133898127067\n",
      "Score for severe_toxic is 0.9848534035920431\n",
      "Score for obscene    is 0.9850845071108659\n",
      "Score for threat     is 0.9873402246798102\n",
      "Score for insult     is 0.9798890140128306\n",
      "Score for identity_hate is 0.9769750074770984\n",
      "Score : 0.9815460091666193\n"
     ]
    }
   ],
   "source": [
    "trained_clfs, scores = training(text, clf, max_iter=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = trained_clfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = test_labels['toxic'] != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tfidf.transform(test[selection]['comment_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = cc.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5         0\n",
       "7         0\n",
       "11        0\n",
       "13        0\n",
       "14        0\n",
       "         ..\n",
       "153150    0\n",
       "153151    0\n",
       "153154    0\n",
       "153155    1\n",
       "153156    0\n",
       "Name: toxic, Length: 63978, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[selection]['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9377442245771984"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(aa, test_labels[selection]['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.9377442245771984\n",
      "severe_toxic 0.9931695270249148\n",
      "obscene 0.9672231079433555\n",
      "threat 0.9969677076495045\n",
      "insult 0.9646440964081403\n",
      "identity_hate 0.9905123636249961\n",
      "Result :  0.9750435045380182\n"
     ]
    }
   ],
   "source": [
    "test_cols = [ 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "res = 0\n",
    "for index,cccc in enumerate(trained_clfs):\n",
    "    aa = cccc.predict(test_input)\n",
    "    acc = metrics.accuracy_score(aa, test_labels[selection][test_cols[index]])\n",
    "    res += acc\n",
    "    print(test_cols[index], acc)\n",
    "\n",
    "print(\"Result : \", res/len(test_cols))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dic = {\n",
    "    \"models\" : {\n",
    "        \"toxic\" : trained_clfs[0],\n",
    "        'severe_toxic' : trained_clfs[1], \n",
    "        'obscene' : trained_clfs[2], \n",
    "        'threat' : trained_clfs[3], \n",
    "        'insult' : trained_clfs[4], \n",
    "        'identity_hate' : trained_clfs[5]\n",
    "    },\n",
    "    \"transform\" : tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': {'toxic': LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=600,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  'severe_toxic': LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=600,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  'obscene': LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=600,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  'threat': LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=600,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  'insult': LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=600,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  'identity_hate': LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=600,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)},\n",
       " 'transform': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0,\n",
       "                 max_features=100000, min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                 preprocessor=None, smooth_idf=True, stop_words=None,\n",
       "                 strip_accents='unicode', sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>,\n",
       "                 use_idf=True, vocabulary=None)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pickles/model.pickle\", 'wb') as file:\n",
    "    pickle.dump(save_dic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
